{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import *\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sbn\n",
    "import os\n",
    "from functools import cmp_to_key\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from deepctr.inputs import SparseFeat, DenseFeat, VarLenSparseFeat, get_feature_names\n",
    "from deepctr.models import DeepFM\n",
    "import pickle\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict_from_file(file, type='default', skiprow = 0):\n",
    "    data = list()\n",
    "    size = 0\n",
    "    with open(file, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            size += 1\n",
    "            try:\n",
    "                if size <= skiprow:\n",
    "                    continue\n",
    "                line = line.replace('\"', '').strip()\n",
    "                if len(line) <= 0:\n",
    "                    continue\n",
    "                if type == 'interet':\n",
    "                    parts = line.split(\",\")\n",
    "                    name = parts[0]\n",
    "                    size = int(parts[1])\n",
    "                    if size > 10 and len(name) > 0:\n",
    "                        data.append(name)\n",
    "                elif type == 'loc':\n",
    "                    parts = line.split(\",\")\n",
    "                    province = parts[0]\n",
    "                    city = parts[1]\n",
    "                    area = parts[2]\n",
    "                    data.append(province)\n",
    "                    data.append(province + \"_\" + city)\n",
    "                    data.append(province + \"_\" + city + \"_\" + area)\n",
    "                elif type == 'publisher':\n",
    "                    parts = line.split(\",\")\n",
    "                    name = parts[0]\n",
    "                    size = int(parts[1])\n",
    "                    if size > 10:\n",
    "                        data.append(name)\n",
    "                else:\n",
    "                    data.append(line)\n",
    "            except:\n",
    "                print(line)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict(dir, type='default', skiprow = 0):\n",
    "    data = list()\n",
    "    size = 0\n",
    "    files = os.listdir(dir)\n",
    "    for file in files:\n",
    "        if not file.endswith(\".csv\"):\n",
    "            continue\n",
    "        data+=load_dict_from_file(dir + \"/\" + file, type, skiprow)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbe_file = 'lbe.pickle'\n",
    "data_map_file = 'data_map.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pickle_data(file, data):\n",
    "    f = open(file, 'wb')\n",
    "    pickle.dump(data, f)\n",
    "    f.close()\n",
    "\n",
    "\n",
    "def load_pickle_data(file):\n",
    "    try:\n",
    "        f1 = open(file, 'rb')\n",
    "        return pickle.load(f1)\n",
    "    except:\n",
    "        pass\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sky game\n",
      "中国新闻网,绥芬河政府网站,1\n",
      "Ansun Biopharma, Inc.,1\n"
     ]
    }
   ],
   "source": [
    "its = load_dict('/home/recsys/dataset/dict/interets', 'interet')\n",
    "locs = load_dict('/home/recsys/dataset/dict/loc', 'loc')\n",
    "publishers = load_dict('/home/recsys/dataset/dict/publisher', 'publisher', 1)\n",
    "cates = load_dict_from_file('/home/recsys/dataset/dict/cate.csv', 'cate', 1)\n",
    "channels = load_dict_from_file('/home/recsys/dataset/dict/channel.csv', 'channel', 1)\n",
    "publishers.append('other')\n",
    "channels.append('')\n",
    "\n",
    "u_levels = [str(i) for i in range(0, 10)]\n",
    "media_levels = [str(i) for i in range(0, 10)]\n",
    "\n",
    "vocabs = dict()\n",
    "vocabs['u_level'] = u_levels\n",
    "vocabs['t_channel'] = channels\n",
    "vocabs['cp_l1_category'] = cates\n",
    "vocabs['cp_publisher'] = publishers\n",
    "vocabs['cp_media_level'] = media_levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data from cache\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(lbe_file) and os.path.exists(data_map_file):\n",
    "    print('load data from cache')\n",
    "    lbe_pickle = load_pickle_data(lbe_file)\n",
    "    data_map_pickle = load_pickle_data(data_map_file)\n",
    "\n",
    "    if lbe_pickle:\n",
    "        lbes = lbe_pickle\n",
    "    if data_map_pickle:\n",
    "        rschannlemap = data_map_pickle['rschannlemap']\n",
    "        itsmap = data_map_pickle['itsmap']\n",
    "        locmap = data_map_pickle['locmap']\n",
    "else:\n",
    "    def gen_label_encode(vocab):\n",
    "        lbe = LabelEncoder()\n",
    "        lbe.fit(vocab)\n",
    "        return lbe\n",
    "\n",
    "\n",
    "    lbes = dict()\n",
    "    for key in vocabs.keys():\n",
    "        lbes[key] = gen_label_encode(vocabs[key])\n",
    "\n",
    "    for key in vocabs.keys():\n",
    "        print(len(vocabs[key]))\n",
    "\n",
    "\n",
    "    def gen_dict_map(vocad):\n",
    "        its_index = dict()\n",
    "        size = 0\n",
    "        for i in vocad:\n",
    "            if i not in its_index.keys():\n",
    "                its_index[i] = size\n",
    "                size += 1\n",
    "        return its_index\n",
    "\n",
    "\n",
    "    rschannles = [str(i) for i in range(1, 33)]\n",
    "    rschannlemap = gen_dict_map(rschannles)\n",
    "\n",
    "    itsmap = gen_dict_map(its)\n",
    "    locmap = gen_dict_map(locs)\n",
    "\n",
    "    data_map = {}\n",
    "    data_map['rschannlemap'] = rschannlemap\n",
    "    data_map['itsmap'] = itsmap\n",
    "    data_map['locmap'] = locmap\n",
    "    save_pickle_data(lbe_file, lbes)\n",
    "    save_pickle_data(data_map_file, data_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '/home/recsys/dataset/train_csv_v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_sort_files(dir):\n",
    "    def compare(x, y):\n",
    "        stat_x = int(x.replace('-', ''))\n",
    "        stat_y = int(y.replace('-', ''))\n",
    "        if stat_x < stat_y:\n",
    "            return -1\n",
    "        elif stat_x > stat_y:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    items = os.listdir(dir)\n",
    "    items.sort(key = cmp_to_key(compare))\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2020-05-21-18', '2020-05-21-19', '2020-05-21-20', '2020-05-21-21', '2020-05-21-22', '2020-05-21-23', '2020-05-22-00', '2020-05-22-01', '2020-05-22-02', '2020-05-22-03', '2020-05-22-04', '2020-05-22-05', '2020-05-22-06', '2020-05-22-07', '2020-05-22-08', '2020-05-22-09', '2020-05-22-10', '2020-05-22-11', '2020-05-22-12', '2020-05-22-13', '2020-05-22-14', '2020-05-22-15', '2020-05-22-16', '2020-05-22-17', '2020-05-22-18', '2020-05-22-19', '2020-05-22-20', '2020-05-22-21', '2020-05-22-22', '2020-05-22-23', '2020-05-23-00', '2020-05-23-01', '2020-05-23-02', '2020-05-23-03', '2020-05-23-04', '2020-05-23-05', '2020-05-23-06', '2020-05-23-07', '2020-05-23-08', '2020-05-23-09', '2020-05-23-10', '2020-05-23-11', '2020-05-23-12', '2020-05-23-13', '2020-05-23-14', '2020-05-23-15', '2020-05-23-16', '2020-05-23-17', '2020-05-23-18', '2020-05-23-19', '2020-05-23-20', '2020-05-23-21', '2020-05-23-22', '2020-05-23-23', '2020-05-24-00', '2020-05-24-01', '2020-05-24-02', '2020-05-24-03', '2020-05-24-04', '2020-05-24-05', '2020-05-24-06', '2020-05-24-07', '2020-05-24-08', '2020-05-24-09', '2020-05-24-10', '2020-05-24-11', '2020-05-24-12', '2020-05-24-13', '2020-05-24-14', '2020-05-24-15', '2020-05-24-16', '2020-05-24-17', '2020-05-24-18', '2020-05-24-19', '2020-05-24-20', '2020-05-24-21', '2020-05-24-22', '2020-05-24-23', '2020-05-25-00', '2020-05-25-01', '2020-05-25-02', '2020-05-25-03', '2020-05-25-04', '2020-05-25-05', '2020-05-25-06', '2020-05-25-07', '2020-05-25-08', '2020-05-25-09', '2020-05-25-10', '2020-05-25-11', '2020-05-25-12', '2020-05-25-13', '2020-05-25-14', '2020-05-25-15']\n"
     ]
    }
   ],
   "source": [
    "print(list_sort_files(dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpus(path):\n",
    "    files = os.listdir(path)\n",
    "    final_file = None\n",
    "    for file in files:\n",
    "        if file.endswith('.csv'):\n",
    "            final_file = path +\"/\"+file\n",
    "            break\n",
    "    if not final_file:\n",
    "        return None\n",
    "    print('load data from ', final_file)\n",
    "    return pd.read_csv(final_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser_publisher(item):\n",
    "    if item not in publishers:\n",
    "        item = 'other'\n",
    "    return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans_data(item):\n",
    "    item['u_umi'] = item['u_umi'].fillna('')\n",
    "    item['u_umi_weight'] = item['u_umi_weight'].astype('str').fillna('')\n",
    "    item['u_uli'] = item['u_uli'].fillna('')\n",
    "    item['u_uli_weight'] = item['u_uli_weight'].astype('str').fillna('')\n",
    "    item['u_usi'] = item['u_usi'].fillna('')\n",
    "    item['u_usi_weight'] = item['u_usi_weight'].astype('str').fillna('')\n",
    "    item['u_level'] = item['u_level'].fillna(0).astype('int').astype('str')\n",
    "    item['cp_media_level'] = item['cp_media_level'].fillna(0).astype('int').astype('str')\n",
    "    item['cp_location'] = item['cp_location'].fillna(\"\")\n",
    "    item['rs_channel'] = item['rs_channel'].fillna(\"\")\n",
    "    item['cp_interests'] = item['cp_interests'].fillna(\"\")\n",
    "    item['rs_p1_score'] = item['rs_p1_score'].fillna(0)\n",
    "    item['rs_gactr'] = item['rs_gactr'].fillna(0)\n",
    "    item['rs_taginfo'] = item['rs_taginfo'].fillna(\"\")\n",
    "    item['rs_taginfo_weight'] = item['rs_taginfo_weight'].fillna(\"\")\n",
    "    item['rs_dactr'] = item['rs_dactr'].fillna(\"\")\n",
    "    item['cp_publisher'] = item['cp_publisher'].apply(parser_publisher)\n",
    "    item['t_channel'] = item['t_channel'].fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_weight(x):\n",
    "    key_ans = x.split(',')\n",
    "    return list([float(item) for item in key_ans])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_pad_seq(values, weights, key2index, max_len=None):\n",
    "    def split(x):\n",
    "        vkeys = list()\n",
    "        try:\n",
    "            key_ans = x.split(',')\n",
    "            for key in key_ans:\n",
    "                if key in key2index:\n",
    "                    vkeys.append(key)\n",
    "        except:\n",
    "            pass\n",
    "        return list(map(lambda x: key2index[x], vkeys))\n",
    "    def split_weight(x):\n",
    "        res = list()\n",
    "        size = 0\n",
    "        parts = x.split(',')\n",
    "        for part in parts:\n",
    "            try:\n",
    "                if part != 'nan':\n",
    "                    res.append(float(part))\n",
    "                else:\n",
    "                    res.append(0)\n",
    "                size += 1\n",
    "            except:\n",
    "                res.append(0)\n",
    "                size += 1\n",
    "        while size < max_len:\n",
    "            res.append(0)\n",
    "            size += 1\n",
    "        return res\n",
    "    index_list = list(map(split, values))\n",
    "    weight_list = None\n",
    "    if weights is not None:\n",
    "        weight_list = list(map(split_weight, weights))\n",
    "    index_list = pad_sequences(index_list, maxlen=max_len, padding='post', )\n",
    "    return index_list, weight_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data from  /home/recsys/dataset/train_csv/2020-05-08-00/train/part-00000-752e58fb-2c30-401c-bd01-b3bcdf0a00e8-c000.csv\n",
      "load data from  /home/recsys/dataset/train_csv/2020-05-08-00/test/part-00000-ff73b820-51e9-4117-a98b-11990ce5a955-c000.csv\n"
     ]
    }
   ],
   "source": [
    "file = '2020-05-08-00'\n",
    "train_corpus = load_corpus('/home/recsys/dataset/train_csv/' + file + '/train')\n",
    "test_corpus = load_corpus('/home/recsys/dataset/train_csv/' + file + '/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_data(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "usi_list, usi_list_weight = gen_pad_seq(train_corpus['u_usi'], train_corpus['u_usi_weight'], itsmap, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 选择特征列 \n",
    "uid, u_umi, u_umi_weight, u_uli, u_uli_weight, u_usi, u_usi_weight, u_level\n",
    "\n",
    "t_channel, t_location\n",
    "\n",
    "item_id, cp_l1_category, cp_interests, cp_location, cp_publisher, cp_media_level, cp_life_hour\n",
    "\n",
    "rs_channel, rs_gactr, rs_taginfo, rs_taginfo_weight, rs_dactr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_features = ['u_level', 't_channel', 'cp_l1_category', 'cp_publisher', 'cp_media_level']\n",
    "dense_features = ['rs_gactr']\n",
    "target = ['action']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = list_sort_files(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "uli_len = 500\n",
    "umi_len = 500\n",
    "usi_len = 100\n",
    "rs_tag_len = 10\n",
    "cp_i_len = 10\n",
    "cp_loc_len = 2\n",
    "t_loc_len = 2\n",
    "rs_channel_len = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_info = list()\n",
    "var_info.append({'label':'u_uli', 'len':uli_len, 'map':itsmap, 'weight':'u_uli_weight'})\n",
    "var_info.append({'label':'u_umi', 'len':umi_len, 'map':itsmap, 'weight':'u_umi_weight'})\n",
    "var_info.append({'label':'u_usi', 'len':usi_len, 'map':itsmap, 'weight':'u_usi_weight'})\n",
    "var_info.append({'label':'rs_taginfo', 'len':rs_tag_len, 'map':itsmap, 'weight':'rs_taginfo_weight'})\n",
    "var_info.append({'label':'cp_interests', 'len':cp_i_len, 'map':itsmap, 'weight':None})\n",
    "var_info.append({'label':'cp_location', 'len':cp_loc_len, 'map':locmap, 'weight':None})\n",
    "var_info.append({'label':'t_location', 'len':t_loc_len, 'map':locmap, 'weight':None})\n",
    "var_info.append({'label':'rs_channel', 'len':rs_channel_len, 'map':rschannlemap, 'weight':None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size = 16\n",
    "fixlen_feature_columns = [SparseFeat(feat, vocabulary_size=len(vocabs[feat]), embedding_dim=emb_size)\n",
    "                              for feat in sparse_features] + [DenseFeat(feat, 1, ) for feat in dense_features]\n",
    "varlen_feature_columns = [VarLenSparseFeat(SparseFeat(item['label'], vocabulary_size=len(item['map']) + 1, embedding_dim=emb_size), maxlen=item['len'], combiner='mean', weight_name=item['weight']) for item in var_info]\n",
    "linear_feature_columns = fixlen_feature_columns + varlen_feature_columns\n",
    "dnn_feature_columns = fixlen_feature_columns + varlen_feature_columns\n",
    "feature_names = get_feature_names(linear_feature_columns + dnn_feature_columns)\n",
    "model = DeepFM(linear_feature_columns, dnn_feature_columns, task='binary')\n",
    "model.compile(\"adam\", \"binary_crossentropy\", metrics=['binary_crossentropy', 'binary_accuracy', tf.keras.metrics.AUC()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_by_batch(file):\n",
    "    train_corpus = load_corpus('/home/recsys/dataset/train_csv/' + file + '/train')\n",
    "    test_corpus = load_corpus('/home/recsys/dataset/train_csv/' + file + '/test')\n",
    "    choose_data =  pd.concat([train_corpus, test_corpus])\n",
    "    cur = int(time.time())\n",
    "    trans_data(choose_data)\n",
    "    print(\"trans_data cost\", (int(time.time()) - cur))\n",
    "    for feat in sparse_features:\n",
    "        lbe = lbes[feat]\n",
    "        choose_data[feat] = lbe.transform(choose_data[feat])\n",
    "    cur = int(time.time())\n",
    "    rs_channel_list, _ = gen_pad_seq(choose_data['rs_channel'], None, rschannlemap, 32)\n",
    "    uli_list, uli_list_weight = gen_pad_seq(choose_data['u_uli'], choose_data['u_uli_weight'], itsmap, 500)\n",
    "    umi_list, umi_list_weight = gen_pad_seq(choose_data['u_umi'], choose_data['u_umi_weight'], itsmap, 500)\n",
    "    usi_list, usi_list_weight = gen_pad_seq(choose_data['u_usi'], choose_data['u_usi_weight'], itsmap, 100)\n",
    "    rs_tag_list, rs_tag_weight = gen_pad_seq(choose_data['rs_taginfo'], choose_data['rs_dactr'], itsmap, 10)\n",
    "    cp_i_list, _ = gen_pad_seq(choose_data['cp_interests'], None, itsmap, 10)\n",
    "    cp_loc_list, _ = gen_pad_seq(choose_data['cp_location'], None, locmap, 2)\n",
    "    t_loc_list, _ = gen_pad_seq(choose_data['t_location'], None, locmap, 2)\n",
    "    print(\"trans_data cost\", (int(time.time()) - cur))\n",
    "    var_data = list()\n",
    "    var_data.append({'label':'u_uli', 'list':uli_list, 'weight':uli_list_weight})\n",
    "    var_data.append({'label':'u_umi', 'list':umi_list, 'weight':umi_list_weight})\n",
    "    var_data.append({'label':'u_usi', 'list':usi_list, 'weight':usi_list_weight})\n",
    "    var_data.append({'label':'rs_taginfo', 'list':rs_tag_list, 'weight':rs_tag_weight})\n",
    "    var_data.append({'label':'cp_interests', 'list':cp_i_list})\n",
    "    var_data.append({'label':'cp_location', 'list':cp_loc_list})\n",
    "    var_data.append({'label':'t_location', 'list':t_loc_list})\n",
    "    var_data.append({'label':'rs_channel', 'list':rs_channel_list})\n",
    "    print(uli_len, umi_len, usi_len, rs_tag_len, cp_i_len, cp_loc_len, t_loc_len)\n",
    "    \n",
    "    model_input = {name: choose_data[name] for name in feature_names}\n",
    "    for item in var_data:\n",
    "        model_input[item['label']] = item['list']\n",
    "        if item['label'] == 'rs_taginfo':\n",
    "            model_input[item['label']+'_weight'] = item['weight']\n",
    "        elif item['label'] == 'u_uli' or item['label'] == 'u_umi' or item['label'] == 'u_usi':\n",
    "            model_input[item['label']+'_weight'] = item['weight']\n",
    "   \n",
    "    history = model.fit(model_input, choose_data[target].values,\n",
    "                    batch_size=64, epochs=5, verbose=2, validation_split=0.1)\n",
    "    model.save_weights('./checkpoints/'+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train model use data from  2020-05-08-00\n",
      "load data from  /home/recsys/dataset/train_csv/2020-05-08-00/train/part-00000-752e58fb-2c30-401c-bd01-b3bcdf0a00e8-c000.csv\n",
      "load data from  /home/recsys/dataset/train_csv/2020-05-08-00/test/part-00000-ff73b820-51e9-4117-a98b-11990ce5a955-c000.csv\n",
      "trans_data cost 4\n",
      "gen_pad_seq cost 8\n",
      "500 500 100 10 10 2 2\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: (<class 'dict'> containing {\"<class 'str'>\"} keys and {\"<class 'pandas.core.series.Series'>\", '(<class \\'list\\'> containing values of types {\\'(<class \\\\\\'list\\\\\\'> containing values of types {\"<class \\\\\\'int\\\\\\'>\"})\\', \\'(<class \\\\\\'list\\\\\\'> containing values of types {\"<class \\\\\\'float\\\\\\'>\", \"<class \\\\\\'int\\\\\\'>\"})\\'})', '(<class \\'list\\'> containing values of types {\\'(<class \\\\\\'list\\\\\\'> containing values of types {\"<class \\\\\\'float\\\\\\'>\"})\\', \\'(<class \\\\\\'list\\\\\\'> containing values of types {\"<class \\\\\\'int\\\\\\'>\"})\\', \\'(<class \\\\\\'list\\\\\\'> containing values of types {\"<class \\\\\\'float\\\\\\'>\", \"<class \\\\\\'int\\\\\\'>\"})\\'})', \"<class 'numpy.ndarray'>\"} values), <class 'numpy.ndarray'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-2df53b92cc0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train model use data from '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain_by_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-39-f0dc682bfc86>\u001b[0m in \u001b[0;36mtrain_by_batch\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     history = model.fit(model_input, choose_data[target].values,\n\u001b[0;32m---> 41\u001b[0;31m                     batch_size=64, epochs=5, verbose=2, validation_split=0.1)\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./checkpoints/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.10/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.10/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.10/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    531\u001b[0m                      'at same time.')\n\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m   \u001b[0madapter_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_adapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_data_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m   \u001b[0;31m# Handle validation_split, we want to split the data and get the training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.10/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mselect_data_adapter\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    996\u001b[0m         \u001b[0;34m\"Failed to find data adapter that can handle \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    997\u001b[0m         \"input: {}, {}\".format(\n\u001b[0;32m--> 998\u001b[0;31m             _type_name(x), _type_name(y)))\n\u001b[0m\u001b[1;32m    999\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter_cls\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to find data adapter that can handle input: (<class 'dict'> containing {\"<class 'str'>\"} keys and {\"<class 'pandas.core.series.Series'>\", '(<class \\'list\\'> containing values of types {\\'(<class \\\\\\'list\\\\\\'> containing values of types {\"<class \\\\\\'int\\\\\\'>\"})\\', \\'(<class \\\\\\'list\\\\\\'> containing values of types {\"<class \\\\\\'float\\\\\\'>\", \"<class \\\\\\'int\\\\\\'>\"})\\'})', '(<class \\'list\\'> containing values of types {\\'(<class \\\\\\'list\\\\\\'> containing values of types {\"<class \\\\\\'float\\\\\\'>\"})\\', \\'(<class \\\\\\'list\\\\\\'> containing values of types {\"<class \\\\\\'int\\\\\\'>\"})\\', \\'(<class \\\\\\'list\\\\\\'> containing values of types {\"<class \\\\\\'float\\\\\\'>\", \"<class \\\\\\'int\\\\\\'>\"})\\'})', \"<class 'numpy.ndarray'>\"} values), <class 'numpy.ndarray'>"
     ]
    }
   ],
   "source": [
    "for file in files:\n",
    "    print('train model use data from ', file)\n",
    "    train_by_batch(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data from  /home/recsys/dataset/train_csv/2020-05-08-00/train/part-00000-752e58fb-2c30-401c-bd01-b3bcdf0a00e8-c000.csv\n",
      "load data from  /home/recsys/dataset/train_csv/2020-05-08-00/test/part-00000-ff73b820-51e9-4117-a98b-11990ce5a955-c000.csv\n",
      "trans_data cost 4\n",
      "trans_data cost 8\n",
      "500 500 100 10 10 2 2\n"
     ]
    }
   ],
   "source": [
    "file = '2020-05-08-00'\n",
    "train_corpus = load_corpus('/home/recsys/dataset/train_csv/' + file + '/train')\n",
    "test_corpus = load_corpus('/home/recsys/dataset/train_csv/' + file + '/test')\n",
    "choose_data =  pd.concat([train_corpus, test_corpus])\n",
    "cur = int(time.time())\n",
    "trans_data(choose_data)\n",
    "print(\"trans_data cost\", (int(time.time()) - cur))\n",
    "for feat in sparse_features:\n",
    "    lbe = lbes[feat]\n",
    "    choose_data[feat] = lbe.transform(choose_data[feat])\n",
    "cur = int(time.time())\n",
    "rs_channel_list, _ = gen_pad_seq(choose_data['rs_channel'], None, rschannlemap, 32)\n",
    "uli_list, uli_list_weight = gen_pad_seq(choose_data['u_uli'], choose_data['u_uli_weight'], itsmap, 500)\n",
    "umi_list, umi_list_weight = gen_pad_seq(choose_data['u_umi'], choose_data['u_umi_weight'], itsmap, 500)\n",
    "usi_list, usi_list_weight = gen_pad_seq(choose_data['u_usi'], choose_data['u_usi_weight'], itsmap, 100)\n",
    "rs_tag_list, rs_tag_weight = gen_pad_seq(choose_data['rs_taginfo'], choose_data['rs_dactr'], itsmap, 10)\n",
    "cp_i_list, _ = gen_pad_seq(choose_data['cp_interests'], None, itsmap, 10)\n",
    "cp_loc_list, _ = gen_pad_seq(choose_data['cp_location'], None, locmap, 2)\n",
    "t_loc_list, _ = gen_pad_seq(choose_data['t_location'], None, locmap, 2)\n",
    "print(\"trans_data cost\", (int(time.time()) - cur))\n",
    "var_data = list()\n",
    "var_data.append({'label':'u_uli', 'list':uli_list, 'weight':uli_list_weight})\n",
    "var_data.append({'label':'u_umi', 'list':umi_list, 'weight':umi_list_weight})\n",
    "var_data.append({'label':'u_usi', 'list':usi_list, 'weight':usi_list_weight})\n",
    "var_data.append({'label':'rs_taginfo', 'list':rs_tag_list, 'weight':rs_tag_weight})\n",
    "var_data.append({'label':'cp_interests', 'list':cp_i_list})\n",
    "var_data.append({'label':'cp_location', 'list':cp_loc_list})\n",
    "var_data.append({'label':'t_location', 'list':t_loc_list})\n",
    "var_data.append({'label':'rs_channel', 'list':rs_channel_list})\n",
    "print(uli_len, umi_len, usi_len, rs_tag_len, cp_i_len, cp_loc_len, t_loc_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost 0\n"
     ]
    }
   ],
   "source": [
    "cur = int(time.time())\n",
    "model_input = {name: choose_data[name] for name in feature_names}\n",
    "for item in var_data:\n",
    "    model_input[item['label']] = item['list']\n",
    "    if item['label'] == 'rs_taginfo':\n",
    "        model_input[item['label']+'_weight'] = item['weight']\n",
    "    elif item['label'] == 'u_uli' or item['label'] == 'u_umi' or item['label'] == 'u_usi':\n",
    "        model_input[item['label']+'_weight'] = item['weight']\n",
    "print('cost', (int(time.time()) - cur))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input['u_usi_weight'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(model_input, choose_data[target].values,\n",
    "                batch_size=64, epochs=5, verbose=2, validation_split=0.1)\n",
    "model.save_weights('./checkpoints/'+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_input' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-79ebfc5a4594>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model_input' is not defined"
     ]
    }
   ],
   "source": [
    "model_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f1db24615f8>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('./checkpoints/'+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint\t\t     cp.ckpt.data-00001-of-00002\n",
      "cp.ckpt.data-00000-of-00002  cp.ckpt.index\n"
     ]
    }
   ],
   "source": [
    "!ls {checkpoint_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import feature_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_function(example_proto):\n",
    "    dics = {'action': tf.io.FixedLenFeature(shape=(), dtype=tf.int64, default_value=6),\n",
    "            'uid': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=''),\n",
    "            'item_id': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=''),\n",
    "#             't_scene': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=''),\n",
    "#             't_location': tf.io.VarLenFeature(dtype=tf.string),\n",
    "#             't_action': tf.io.FixedLenFeature(shape=(), dtype=tf.int64, default_value=1),\n",
    "#             't_channel': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=''),\n",
    "#             't_scene': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=''),\n",
    "#             'u_level': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=''),\n",
    "#             'u_uli': tf.io.VarLenFeature(dtype=tf.string),\n",
    "#             'u_umi': tf.io.VarLenFeature(dtype=tf.string),\n",
    "#             'u_usi': tf.io.VarLenFeature(dtype=tf.string),\n",
    "#             'cp_word_count': tf.io.FixedLenFeature(shape=(), dtype=tf.int64, default_value=0),\n",
    "#             'cp_media_level': tf.io.FixedLenFeature(shape=(), dtype=tf.int64, default_value=0),\n",
    "#             'cp_publisher': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=''),\n",
    "#             'cp_is_local_publisher': tf.io.FixedLenFeature(shape=(), dtype=tf.int64, default_value=0),\n",
    "#             'cp_location': tf.io.VarLenFeature(dtype=tf.string),\n",
    "#             'cp_is_local': tf.io.FixedLenFeature(shape=(), dtype=tf.int64, default_value=0),\n",
    "#             'cp_l1_category': tf.io.VarLenFeature(dtype=tf.string),\n",
    "#             'cp_life_hour': tf.io.FixedLenFeature(shape=(), dtype=tf.int64, default_value=72),\n",
    "#             'cp_interests': tf.io.VarLenFeature(dtype=tf.string),\n",
    "#             'rs_gactr': tf.io.FixedLenFeature(shape=(), dtype=tf.float32, default_value=0),\n",
    "#             'rs_channel': tf.io.VarLenFeature(dtype=tf.string),\n",
    "            'rs_taginfo': tf.io.VarLenFeature(dtype=tf.string),\n",
    "           }\n",
    "    # parse all features in a single example according to the dics\n",
    "    parsed_example = tf.io.parse_single_example(example_proto, dics)\n",
    "    target = parsed_example['action']\n",
    "    feature_columns = []\n",
    "    \n",
    "    del parsed_example['action']\n",
    "    return parsed_example, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = train.map(parse_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item: SparseTensor(indices=tf.Tensor(\n",
      "[[0]\n",
      " [1]\n",
      " [2]\n",
      " [3]], shape=(4, 1), dtype=int64), values=tf.Tensor(\n",
      "[b'\\xe7\\xbe\\x8e\\xe5\\x9b\\xbd'\n",
      " b'\\xe5\\x9b\\xbd\\xe9\\x99\\x85\\xe6\\x97\\xb6\\xe6\\x94\\xbf'\n",
      " b'\\xe4\\xbf\\x84\\xe7\\xbd\\x97\\xe6\\x96\\xaf' b'\\xe5\\x9b\\xbd\\xe9\\x99\\x85'], shape=(4,), dtype=string), dense_shape=tf.Tensor([4], shape=(1,), dtype=int64))\n",
      "target tf.Tensor(0, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for item, target in new_dataset.take(1):\n",
    "    print('item:', item['rs_taginfo'])\n",
    "    print('target', target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: ({rs_taginfo: (None,), item_id: (), uid: ()}, ()), types: ({rs_taginfo: tf.string, item_id: tf.string, uid: tf.string}, tf.int64)>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_columns = ['sex', 'n_siblings_spouses', 'parch', 'class', 'deck', 'embark_town', 'alone']\n",
    "numeric_columns = ['age', 'fare']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
