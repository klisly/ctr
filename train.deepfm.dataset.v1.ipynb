{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import *\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sbn\n",
    "import os\n",
    "from functools import cmp_to_key\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from deepctr.inputs import SparseFeat, DenseFeat, VarLenSparseFeat, get_feature_names\n",
    "from deepctr.models import DeepFM\n",
    "import pickle\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "from tensorflow import feature_column\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import sys\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "3.6\n"
     ]
    }
   ],
   "source": [
    "print(tf.version.VERSION)\n",
    "print('{}.{}'.format(sys.version_info.major,sys.version_info.minor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict_from_file(file, type='default', skiprow = 0):\n",
    "    data = list()\n",
    "    size = 0\n",
    "    with open(file, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            size += 1\n",
    "            try:\n",
    "                if size <= skiprow:\n",
    "                    continue\n",
    "                line = line.replace('\"', '').strip()\n",
    "                if len(line) <= 0:\n",
    "                    continue\n",
    "                if type == 'interet':\n",
    "                    parts = line.split(\",\")\n",
    "                    name = parts[0]\n",
    "                    size = int(parts[1])\n",
    "                    if size > 10 and len(name) > 0:\n",
    "                        data.append(name)\n",
    "                elif type == 'loc':\n",
    "                    parts = line.split(\",\")\n",
    "                    province = parts[0]\n",
    "                    city = parts[1]\n",
    "                    area = parts[2]\n",
    "                    data.append(province)\n",
    "                    data.append(province + \"_\" + city)\n",
    "                    data.append(province + \"_\" + city + \"_\" + area)\n",
    "                elif type == 'publisher':\n",
    "                    parts = line.split(\",\")\n",
    "                    name = parts[0]\n",
    "                    size = int(parts[1])\n",
    "                    if size > 10:\n",
    "                        data.append(name)\n",
    "                else:\n",
    "                    data.append(line)\n",
    "            except:\n",
    "                print(line)\n",
    "    return data\n",
    "\n",
    "def load_dict(dir, type='default', skiprow = 0):\n",
    "    data = list()\n",
    "    size = 0\n",
    "    files = os.listdir(dir)\n",
    "    for file in files:\n",
    "        if not file.endswith(\".csv\"):\n",
    "            continue\n",
    "        data+=load_dict_from_file(dir + \"/\" + file, type, skiprow)\n",
    "    return list(set(data))\n",
    "\n",
    "def save_pickle_data(file, data):\n",
    "    f = open(file, 'wb')\n",
    "    pickle.dump(data, f)\n",
    "    f.close()\n",
    "\n",
    "\n",
    "def load_pickle_data(file):\n",
    "    try:\n",
    "        f1 = open(file, 'rb')\n",
    "        return pickle.load(f1)\n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "lbe_file = 'lbe.pickle'\n",
    "data_map_file = 'data_map.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '/data10t/mgf/datas/info'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please check the latest version manually on https://pypi.org/project/deepctr/#history\n",
      "Sky game\n",
      "中国新闻网,绥芬河政府网站,1\n",
      "Ansun Biopharma, Inc.,1\n"
     ]
    }
   ],
   "source": [
    "its = load_dict(dir + '/dict/interets', 'interet')\n",
    "locs = load_dict(dir + '/dict/loc', 'loc')\n",
    "publishers = load_dict(dir + '/dict/publisher', 'publisher', 1)\n",
    "cates = load_dict_from_file(dir + '/dict/cate.csv', 'cate', 1)\n",
    "channels = load_dict_from_file(dir + '/dict/channel.csv', 'channel', 1)\n",
    "publishers.append('other')\n",
    "channels.append('')\n",
    "\n",
    "u_levels = [str(i) for i in range(0, 10)]\n",
    "media_levels = [str(i) for i in range(0, 10)]\n",
    "rschannles = [str(i) for i in range(1, 33)]\n",
    "vocabs = dict()\n",
    "vocabs['u_level'] = u_levels\n",
    "vocabs['t_channel'] = channels\n",
    "vocabs['cp_l1_category'] = cates\n",
    "vocabs['cp_publisher'] = publishers\n",
    "vocabs['cp_media_level'] = media_levels\n",
    "vocabs['rschannles'] = rschannles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dir_path):\n",
    "    files = os.listdir(dir_path)\n",
    "    print(dir_path)\n",
    "    dataset = tf.data.TFRecordDataset(filenames = [dir_path + '/' + file for file in files], num_parallel_reads= tf.data.experimental.AUTOTUNE )\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = dir + \"/corpus/2020-05-25-16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data10t/mgf/datas/info/corpus/2020-05-25-16/train\n",
      "/data10t/mgf/datas/info/corpus/2020-05-25-16/test\n"
     ]
    }
   ],
   "source": [
    "train = load_dataset(path+\"/train\")\n",
    "test = load_dataset(path+\"/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.core.example.example_pb2.Example'>\n"
     ]
    }
   ],
   "source": [
    "for raw_record in train.take(1):\n",
    "    example = tf.train.Example()\n",
    "    example.ParseFromString(raw_record.numpy())\n",
    "    print(type(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用于创建一个特征列\n",
    "# 并转换一批次数据的一个实用程序方法\n",
    "def demo(feature_column):\n",
    "    feature_layer = layers.DenseFeatures(feature_column)\n",
    "    return feature_layer(example_batch).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 选择特征列 \n",
    "uid, u_umi, u_umi_weight, u_uli, u_uli_weight, u_usi, u_usi_weight, u_level\n",
    "\n",
    "t_channel, t_location\n",
    "\n",
    "item_id, cp_l1_category, cp_interests, cp_location, cp_publisher, cp_media_level, cp_life_hour\n",
    "\n",
    "rs_channel, rs_gactr, rs_taginfo, rs_taginfo_weight, rs_dactr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "class NumpyFunctionHelper:\n",
    "    def __init__(self, maxlen=None, dtype='int32', padding='post', truncating='post', value=0.0):\n",
    "        self.maxlen = maxlen\n",
    "        self.dtype = dtype\n",
    "        self.padding = padding\n",
    "        self.truncating = truncating\n",
    "        self.value = value\n",
    "        self.out_dtype = tf.int32\n",
    "        if self.dtype == 'int64':\n",
    "            self.out_dtype = tf.int64\n",
    "        elif self.dtype == 'float32':\n",
    "            self.out_dtype = tf.float32\n",
    "        elif self.dtype == 'str':\n",
    "            self.out_dtype = tf.string\n",
    "        return\n",
    "\n",
    "    def pad_sequences(self, x):\n",
    "        # x will be a numpy array with the contents of the input to the\n",
    "        # tf.function\n",
    "        # note: x will be processed as a list of sequences\n",
    "        return sequence.pad_sequences(x,\n",
    "                                      maxlen = self.maxlen,\n",
    "                                      dtype = self.dtype,\n",
    "                                      padding = self.padding,\n",
    "                                      truncating = self.truncating,\n",
    "                                      value = self.value)\n",
    "\n",
    "    # tf.numpy_function用于将一个numpy函数转换为一个tensor的operator，以便嵌入到计算图中处理Tensor\n",
    "    # 构造一个能支持Tensor的填充截断函数\n",
    "    # 调用方法NumpyFunctionHelper.tf_pad_sequences(helper, in_tensor)\n",
    "    @tf.function(experimental_relax_shapes=True)\n",
    "    def tf_pad_sequences(self, in_tensor):\n",
    "        y = tf.numpy_function(self.pad_sequences, [in_tensor], self.out_dtype)\n",
    "        return y\n",
    "\n",
    "    # 毫秒时间戳转换为local的struct_time\n",
    "    def timestamp_to_time(self, ts):\n",
    "        st = time.localtime(ts/1000)\n",
    "        return tf.constant([st.tm_mon, st.tm_mday, st.tm_hour, st.tm_min, st.tm_wday])\n",
    "\n",
    "    @tf.function(experimental_relax_shapes=True)\n",
    "    def tf_timestamp_to_time(self, ts):\n",
    "        y = tf.py_function(self.timestamp_to_time, ts, self.out_dtype)\n",
    "        return y\n",
    "    \n",
    "    def pad_float_sequences(self, x):\n",
    "        lst = x.tolist()\n",
    "        res = []\n",
    "        if len(x) < self.maxlen:\n",
    "            for i in range(self.maxlen - len(x)):\n",
    "                res.append(0.0)\n",
    "\n",
    "        return np.asarray(lst + res, dtype=np.float32)[:self.maxlen]\n",
    "    \n",
    "    @tf.function(experimental_relax_shapes=True)\n",
    "    def tf_pad_float_sequences(self, in_tensor):\n",
    "        y = tf.numpy_function(self.pad_float_sequences, [in_tensor], tf.float32)\n",
    "        return y\n",
    "    \n",
    "    def pad_str_sequences(self, x):\n",
    "        lst = x.tolist()\n",
    "        res = []\n",
    "        if len(x) < self.maxlen:\n",
    "            for i in range(self.maxlen - len(x)):\n",
    "                res.append(b'<PAD>')\n",
    "\n",
    "        return np.asarray(lst + res)[:self.maxlen]\n",
    "    \n",
    "    @tf.function(experimental_relax_shapes=True)\n",
    "    def tf_pad_str_sequences(self, in_tensor):\n",
    "        y = tf.numpy_function(self.pad_str_sequences, [in_tensor], tf.string)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "spc = \"t_location:3;u_uli_interest:150;u_uli_interest_weight:150;u_uli_cate:20;u_uli_cate_weight:20;\"\\\n",
    "+\"u_umi_interest:150;u_umi_interest_weight:150;u_umi_cate:20;u_umi_cate_weight:20;\"\\\n",
    "+\"u_usi_interest:15;u_usi_interest_weight:15;u_usi_cate:5;u_usi_cate_weight:5;\"\\\n",
    "+\"cp_category:3;cp_category_weight:3;cp_interests:8;cp_interests_weight:8;cp_location:3;cp_location_weight:3;\"\\\n",
    "+\"rs_channel:32;\"\\\n",
    "+\"rs_tag_interest:8;rs_tag_interest_dactr:8;rs_tag_cate:3;rs_tag_cate_dactr:3\"\n",
    "sparse_configs = [{'key':item.split(\":\")[0], 'max_len':int(item.split(\":\")[1])} for item in spc.split(\";\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for config in sparse_configs:\n",
    "    key = config['key']\n",
    "    max_len = config['max_len']\n",
    "    dtype=tf.string\n",
    "    value=b'<PAD>'\n",
    "    if key.endswith('weight') or key.endswith('dactr'):\n",
    "        dtype=tf.float32\n",
    "        value=0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every feature: ['cp_clickbait_type', 'cp_is_local', 'cp_is_local_publisher', 'cp_life_hour', 'cp_list_image_count', 'cp_media_level', 'cp_newsy_score', 'cp_product_type', 'cp_publish_time', 'cp_publisher', 'cp_vulgar_type', 'cp_word_count', 'item_id', 'report_time', 'request_id', 'rs_gactr', 'rs_p1_score', 't_action', 't_channel', 't_country', 't_ctype', 't_ip', 't_language', 't_pid', 't_scene', 'u_level', 'uid']\n",
      "A batch of request_id: tf.Tensor(\n",
      "[b'b4c8ae142d22e70f82fb41d0a8317fa76b91e25b'\n",
      " b'abf297053a6e3002eed81d52349b8bce81dd4344'], shape=(2,), dtype=string)\n",
      "A batch of cp_clickbait_type: tf.Tensor([0 0], shape=(2,), dtype=int64)\n",
      "A batch of targets: tf.Tensor([0 0], shape=(2,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "def parse_function(example_proto):\n",
    "    dics = {\n",
    "            'request_id':tf.io.FixedLenFeature(shape=(),dtype=tf.string, default_value=''),\n",
    "            'report_time': tf.io.FixedLenFeature(shape=(), dtype=tf.int64, default_value=0),\n",
    "            't_language': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=''),\n",
    "            't_country': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=''),\n",
    "            't_pid': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=''),\n",
    "            't_ip': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=''),\n",
    "            't_channel': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=''),\n",
    "            't_scene': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=''),\n",
    "            't_action': tf.io.FixedLenFeature(shape=(), dtype=tf.int64, default_value=0),\n",
    "            't_ctype': tf.io.FixedLenFeature(shape=(), dtype=tf.int64, default_value=0),\n",
    "#             't_location': tf.io.VarLenFeature(dtype=tf.string),\n",
    "            'uid': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=''),\n",
    "            'u_level': tf.io.FixedLenFeature(shape=(), dtype=tf.int64, default_value=0),\n",
    "#             'u_uli_interest': tf.io.VarLenFeature(dtype=tf.string),\n",
    "#             'u_uli_interest_weight': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "#             'u_uli_cate': tf.io.VarLenFeature(dtype=tf.string),\n",
    "#             'u_uli_cate_weight': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "#             'u_umi_interest': tf.io.VarLenFeature(dtype=tf.string),\n",
    "#             'u_umi_interest_weight': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "#             'u_umi_cate': tf.io.VarLenFeature(dtype=tf.string),\n",
    "#             'u_umi_cate_weight': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "#             'u_usi_interest': tf.io.VarLenFeature(dtype=tf.string),\n",
    "#             'u_usi_interest_weight': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "#             'u_usi_cate': tf.io.VarLenFeature(dtype=tf.string),\n",
    "#             'u_usi_cate_weight': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "            'item_id': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=''),\n",
    "            'cp_publisher': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=''),\n",
    "            'cp_media_level': tf.io.FixedLenFeature(shape=(), dtype=tf.int64, default_value=0),\n",
    "            'cp_publish_time': tf.io.FixedLenFeature(shape=(), dtype=tf.int64, default_value=0),\n",
    "            'cp_life_hour': tf.io.FixedLenFeature(shape=(), dtype=tf.int64, default_value=0),\n",
    "            'cp_newsy_score': tf.io.FixedLenFeature(shape=(), dtype=tf.float32, default_value=0),\n",
    "            'cp_word_count': tf.io.FixedLenFeature(shape=(), dtype=tf.int64, default_value=0),\n",
    "#             'cp_category': tf.io.VarLenFeature(dtype=tf.string),\n",
    "#             'cp_category_weight': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "#             'cp_interests': tf.io.VarLenFeature(dtype=tf.string),\n",
    "#             'cp_interests_weight': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "#             'cp_location': tf.io.VarLenFeature(dtype=tf.string),\n",
    "#             'cp_location_weight': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "            'cp_list_image_count': tf.io.FixedLenFeature(shape=(), dtype=tf.int64, default_value=0),\n",
    "            'cp_clickbait_type': tf.io.FixedLenFeature(shape=(), dtype=tf.int64, default_value=0),\n",
    "            'cp_vulgar_type': tf.io.FixedLenFeature(shape=(), dtype=tf.int64, default_value=0),\n",
    "            'cp_product_type': tf.io.FixedLenFeature(shape=(), dtype=tf.int64, default_value=0),\n",
    "            'cp_is_local': tf.io.FixedLenFeature(shape=(), dtype=tf.int64, default_value=0),\n",
    "            'cp_is_local_publisher': tf.io.FixedLenFeature(shape=(), dtype=tf.int64, default_value=0),\n",
    "#             'rs_channel': tf.io.VarLenFeature(dtype=tf.string),\n",
    "            'rs_p1_score': tf.io.FixedLenFeature(shape=(), dtype=tf.float32, default_value=0.0),\n",
    "            'rs_gactr': tf.io.FixedLenFeature(shape=(), dtype=tf.float32, default_value=0.0),\n",
    "#             'rs_tag_interest': tf.io.VarLenFeature(dtype=tf.string),\n",
    "#             'rs_tag_interest_dactr': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "#             'rs_tag_cate': tf.io.VarLenFeature(dtype=tf.string),\n",
    "#             'rs_tag_cate_dactr': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "            'action': tf.io.FixedLenFeature(shape=(), dtype=tf.int64, default_value=0),\n",
    "           }\n",
    "    # parse all features in a single example according to the dics\n",
    "    parsed_example = tf.io.parse_single_example(example_proto, dics)\n",
    "#     for config in sparse_configs:\n",
    "#         key = config['key']\n",
    "#         print(key)\n",
    "#         dtype='str'\n",
    "#         value=b'<PAD>'\n",
    "#         if key.endswith('weight') or key.endswith('dactr'):\n",
    "#             dtype=tf.float32\n",
    "#             value=0.0\n",
    "#             h = NumpyFunctionHelper(maxlen=config['max_len'], dtype=dtype, value=value)\n",
    "#             parsed_example[key] = NumpyFunctionHelper.tf_pad_float_sequences(h, tf.sparse.to_dense(parsed_example[key]))\n",
    "#             parsed_example[key].set_shape([config['max_len']])\n",
    "#         else:\n",
    "#             h = NumpyFunctionHelper(maxlen=config['max_len'], dtype=dtype, value=value)\n",
    "#             parsed_example[key] = NumpyFunctionHelper.tf_pad_str_sequences(h, tf.sparse.to_dense(parsed_example[key]))\n",
    "#             parsed_example[key].set_shape([config['max_len']])\n",
    "    \n",
    "    parsed_example['cp_media_level'] = tf.strings.as_string(parsed_example['cp_media_level'])\n",
    "    parsed_example['u_level'] = tf.strings.as_string(parsed_example['u_level'])\n",
    "    target = parsed_example['action']\n",
    "    del parsed_example['action']\n",
    "    return parsed_example, target\n",
    "train_dataset = train.map(parse_function)\n",
    "test_dataset = test.map(parse_function)\n",
    "\n",
    "new_dataset = train_dataset\n",
    "new_dataset = new_dataset.batch(2)\n",
    "example_batch = next(iter(new_dataset))[0]\n",
    "\n",
    "# # feature_column.categorical_column_with_vocabulary_list(key='u_level', vocabulary_list=u_levels, num_oov_buckets=1)\n",
    "for feature_batch, label_batch in new_dataset.take(1):\n",
    "    print('Every feature:', list(feature_batch.keys()))\n",
    "#     print('A batch of u_uli:', feature_batch['u_uli'])\n",
    "    print('A batch of request_id:', feature_batch['request_id'])\n",
    "#     print('A batch of item_id:', feature_batch['u_uli'])\n",
    "    print('A batch of cp_clickbait_type:', feature_batch['cp_clickbait_type'])\n",
    "    print('A batch of targets:', label_batch )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建特征列\n",
    "feature_columns = []\n",
    "feature_layer_inputs = {}\n",
    "real = {\n",
    "    colname : tf.feature_column.numeric_column(colname) \n",
    "          for colname in \n",
    "            ('rs_gactr,rs_p1_score,cp_is_local_publisher,cp_is_local,cp_vulgar_type,cp_clickbait_type' +\n",
    "             ',cp_word_count,cp_newsy_score,cp_life_hour').split(',')\n",
    "}\n",
    "sparse = {\n",
    "    't_channel': tf.feature_column.categorical_column_with_vocabulary_list('t_channel', vocabulary_list=channels),\n",
    "    'uid'   : tf.feature_column.categorical_column_with_hash_bucket('uid', hash_bucket_size=300000),\n",
    "    'u_level' : tf.feature_column.categorical_column_with_vocabulary_list('u_level', vocabulary_list=u_levels),\n",
    "    'item_id'   : tf.feature_column.categorical_column_with_hash_bucket('item_id', hash_bucket_size=300000),\n",
    "    'cp_publisher': tf.feature_column.categorical_column_with_vocabulary_list('cp_publisher', vocabulary_list=publishers),\n",
    "    'cp_media_level' : tf.feature_column.categorical_column_with_vocabulary_list('cp_media_level', vocabulary_list=media_levels),\n",
    "#     'rs_channel': tf.feature_column.categorical_column_with_vocabulary_list('rs_channel', vocabulary_list=rschannles),\n",
    "}\n",
    "\n",
    "inputs = {\n",
    "    colname : tf.keras.layers.Input(name=colname, shape=(1,), dtype='float32') \n",
    "          for colname in real.keys()\n",
    "}\n",
    "inputs.update({\n",
    "    colname : tf.keras.layers.Input(name=colname, shape=(1,), dtype='string') \n",
    "          for colname in sparse.keys()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dimen(col):\n",
    "    if col == 'uid':\n",
    "        return 32\n",
    "    elif col == 'uid':\n",
    "        return 32\n",
    "    elif col == 'cp_publisher':\n",
    "        return 16\n",
    "    else:\n",
    "        return 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#特征工程\n",
    "embed = {\n",
    "       'embed_{}'.format(colname) : tf.feature_column.embedding_column(col, get_dimen(col))\n",
    "          for colname, col in sparse.items()\n",
    "}\n",
    "real.update(embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse = {\n",
    "    colname : tf.feature_column.indicator_column(col)\n",
    "          for colname, col in sparse.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "DNN_HIDDEN_UNITS = '64,32'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib64/python3.6/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4267: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From /usr/local/lib64/python3.6/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4322: VocabularyListCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From /usr/local/lib64/python3.6/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4322: HashedCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n"
     ]
    }
   ],
   "source": [
    "def wide_and_deep_classifier(inputs, linear_feature_columns, dnn_feature_columns, dnn_hidden_units):\n",
    "    deep = tf.keras.layers.DenseFeatures(dnn_feature_columns, name='deep_inputs')(inputs)\n",
    "    layers = [int(x) for x in dnn_hidden_units.split(',')]\n",
    "    for layerno, numnodes in enumerate(layers):\n",
    "        deep = tf.keras.layers.Dense(numnodes, activation='relu', name='dnn_{}'.format(layerno+1))(deep)        \n",
    "    wide = tf.keras.layers.DenseFeatures(linear_feature_columns, name='wide_inputs')(inputs)\n",
    "    both = tf.keras.layers.concatenate([deep, wide], name='both')\n",
    "    output = tf.keras.layers.Dense(1, activation='sigmoid', name='pred')(both)\n",
    "    model = tf.keras.Model(inputs, output)\n",
    "    model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['binary_crossentropy', 'accuracy', tf.keras.metrics.AUC()],\n",
    "              run_eagerly=True)\n",
    "    return model\n",
    "    \n",
    "model = wide_and_deep_classifier(\n",
    "    inputs,\n",
    "    linear_feature_columns = sparse.values(),\n",
    "    dnn_feature_columns = real.values(),\n",
    "    dnn_hidden_units = DNN_HIDDEN_UNITS)\n",
    "tf.keras.utils.plot_model(model, 'wide_and_deep_model.png', show_shapes=False, rankdir='LR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 5000\n",
    "batch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.shuffle(buffer_size).batch(batch_size).repeat(2)\n",
    "test_dataset = test_dataset.shuffle(buffer_size).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1002/1002 [==============================] - 255s 254ms/step - loss: 0.3248 - binary_crossentropy: 0.3248 - accuracy: 0.9002 - auc: 0.6020 - val_loss: 0.3051 - val_binary_crossentropy: 0.3051 - val_accuracy: 0.9021 - val_auc: 0.6692\n",
      "Epoch 2/10\n",
      "1002/1002 [==============================] - 358s 357ms/step - loss: 0.2965 - binary_crossentropy: 0.2965 - accuracy: 0.9003 - auc: 0.7211 - val_loss: 0.3090 - val_binary_crossentropy: 0.3090 - val_accuracy: 0.9022 - val_auc: 0.6661\n",
      "Epoch 3/10\n",
      "1002/1002 [==============================] - 461s 460ms/step - loss: 0.2844 - binary_crossentropy: 0.2844 - accuracy: 0.9003 - auc: 0.7593 - val_loss: 0.3179 - val_binary_crossentropy: 0.3178 - val_accuracy: 0.9022 - val_auc: 0.6469\n",
      "Epoch 4/10\n",
      "1002/1002 [==============================] - 567s 565ms/step - loss: 0.2787 - binary_crossentropy: 0.2787 - accuracy: 0.9004 - auc: 0.7747 - val_loss: 0.3341 - val_binary_crossentropy: 0.3342 - val_accuracy: 0.9022 - val_auc: 0.6354\n",
      "Epoch 5/10\n",
      "1002/1002 [==============================] - 679s 677ms/step - loss: 0.2749 - binary_crossentropy: 0.2748 - accuracy: 0.9003 - auc: 0.7845 - val_loss: 0.3386 - val_binary_crossentropy: 0.3388 - val_accuracy: 0.9022 - val_auc: 0.6217\n",
      "Epoch 6/10\n",
      "   2/1002 [..............................] - ETA: 1:43:33 - loss: 0.2598 - binary_crossentropy: 0.2598 - accuracy: 0.9062 - auc: 0.8002WARNING:tensorflow:Early stopping conditioned on metric `val_auc` which is not available. Available metrics are: loss,binary_crossentropy,accuracy,auc\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[512,632484] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:MatMul]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-221f210844f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m model.fit(train_dataset,\n\u001b[1;32m      3\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m           epochs=10, callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_auc', patience=1)])\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mdistributed_function\u001b[0;34m(input_iterator)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_feed_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     outputs = strategy.experimental_run_v2(\n\u001b[0;32m---> 85\u001b[0;31m         per_replica_function, args=args)\n\u001b[0m\u001b[1;32m     86\u001b[0m     \u001b[0;31m# Out of PerReplica outputs reduce or pick values to return.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     all_outputs = dist_utils.unwrap_output_dict(\n",
      "\u001b[0;32m/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mexperimental_run_v2\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m    761\u001b[0m       fn = autograph.tf_convert(fn, ag_ctx.control_status_ctx(),\n\u001b[1;32m    762\u001b[0m                                 convert_by_default=False)\n\u001b[0;32m--> 763\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extended\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mcall_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   1817\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1818\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1819\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1821\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_call_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   2162\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2163\u001b[0m         replica_id_in_sync_group=constant_op.constant(0, dtypes.int32)):\n\u001b[0;32m-> 2164\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2166\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_reduce_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestinations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    256\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mControlStatusCtx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUNSPECIFIED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mismethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, x, y, sample_weight, class_weight, reset_metrics, standalone)\u001b[0m\n\u001b[1;32m    431\u001b[0m       \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m       \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m       output_loss_metrics=model._output_loss_metrics)\n\u001b[0m\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, inputs, targets, sample_weights, output_loss_metrics)\u001b[0m\n\u001b[1;32m    310\u001b[0m           \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m           \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m           output_loss_metrics=output_loss_metrics))\n\u001b[0m\u001b[1;32m    313\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36m_process_single_batch\u001b[0;34m(model, inputs, targets, output_loss_metrics, sample_weights, training)\u001b[0m\n\u001b[1;32m    267\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backwards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaled_total_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m           \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaled_total_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m           if isinstance(model.optimizer,\n\u001b[1;32m    271\u001b[0m                         loss_scale_optimizer.LossScaleOptimizer):\n",
      "\u001b[0;32m/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1029\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py\u001b[0m in \u001b[0;36m_MatMulGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m   1628\u001b[0m   \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1629\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt_a\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt_b\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1630\u001b[0;31m     \u001b[0mgrad_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_b\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1631\u001b[0m     \u001b[0mgrad_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1632\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt_a\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mt_b\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   5614\u001b[0m         \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5615\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5616\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5617\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5618\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtranspose_a\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6604\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6605\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6606\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6607\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[512,632484] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:MatMul]"
     ]
    }
   ],
   "source": [
    "# 模型训练\n",
    "model.fit(train_dataset,\n",
    "          validation_data=test_dataset,\n",
    "          epochs=10, callbacks=[tf.keras.callbacks.EarlyStopping(monitor='auc', patience=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_columns.append(feature_column.numeric_column(\"rs_gactr\"))\n",
    "# feature_layer_inputs[\"rs_gactr\"] = tf.keras.Input(shape=(1,), name=\"rs_gactr\", dtype='float32')\n",
    "# # item_id_hash = feature_column.categorical_column_with_hash_bucket('item_id', hash_bucket_size=500000)\n",
    "# # item_id_one_hot = feature_column.indicator_column(item_id_hash)\n",
    "# # feature_columns.append(item_id_one_hot)\n",
    "# # feature_layer_inputs['item_id'] = tf.keras.Input(shape=(1,), name='item_id', dtype=tf.string)\n",
    "# # item_id_embedding = feature_column.embedding_column(item_id_hash, dimension=64)\n",
    "# # feature_columns.append(item_id_embedding)\n",
    "\n",
    "# # uid_hash = feature_column.categorical_column_with_hash_bucket('uid', hash_bucket_size=500000)\n",
    "# # uid_embedding = feature_column.embedding_column(uid_hash, dimension=64)\n",
    "# # feature_columns.append(uid_embedding)\n",
    "# # feature_layer_inputs[\"uid\"] = tf.keras.Input(shape=(), name=\"uid\", dtype='string')\n",
    "\n",
    "# # u_uli = feature_column.categorical_column_with_hash_bucket('u_uli', hash_bucket_size=30000, dtype=tf.string)\n",
    "# # u_uli_one_hot = feature_column.indicator_column(u_uli)\n",
    "# # feature_columns.append(u_uli_one_hot)\n",
    "# # feature_layer_inputs[\"u_uli\"] = tf.keras.Input(shape=(300,), name=\"u_uli\", dtype=tf.string)\n",
    "# # u_uli_embedding = feature_column.embedding_column(u_uli, dimension=64)\n",
    "# # feature_columns.append(u_uli_embedding)\n",
    "\n",
    "# # feature_layer_inputs[\"u_uli\"] = tf.keras.Input(shape=(1,), sparse=True, dtype=tf.string, name=\"u_uli\")\n",
    "\n",
    "# # u_umi = feature_column.categorical_column_with_vocabulary_list('u_umi', vocabulary_list=its, num_oov_buckets=1)\n",
    "# # u_umi_embedding = feature_column.embedding_column(u_umi, dimension=64)\n",
    "# # feature_columns.append(u_umi_embedding)\n",
    "\n",
    "# # u_usi = feature_column.categorical_column_with_vocabulary_list('u_usi', vocabulary_list=its, num_oov_buckets=1)\n",
    "# # u_usi_embedding = feature_column.embedding_column(u_usi, dimension=64)\n",
    "# # feature_columns.append(u_usi_embedding)\n",
    "\n",
    "# # u_level = feature_column.categorical_column_with_vocabulary_list('u_level', vocabulary_list=u_levels, num_oov_buckets=1)\n",
    "# # u_level_one_hot = feature_column.indicator_column(u_level)\n",
    "# # feature_columns.append(u_level_one_hot)\n",
    "\n",
    "# # t_channel = feature_column.categorical_column_with_vocabulary_list('t_channel', vocabulary_list=channels, num_oov_buckets=1)\n",
    "# # t_channel_one_hot = feature_column.indicator_column(t_channel)\n",
    "# # feature_columns.append(t_channel_one_hot)\n",
    "\n",
    "# # t_location = feature_column.categorical_column_with_vocabulary_list('t_location', vocabulary_list=locs, num_oov_buckets=1)\n",
    "# # t_location_one_hot = feature_column.indicator_column(t_location)\n",
    "# # feature_columns.append(t_location_one_hot)\n",
    "\n",
    "# # cp_media_level = feature_column.categorical_column_with_vocabulary_list('cp_media_level', vocabulary_list=media_levels, num_oov_buckets=1)\n",
    "# # cp_media_level_one_hot = feature_column.indicator_column(cp_media_level)\n",
    "# # feature_columns.append(cp_media_level_one_hot)\n",
    "\n",
    "# # cp_publisher = feature_column.categorical_column_with_vocabulary_list('cp_publisher', vocabulary_list=publishers, num_oov_buckets=1)\n",
    "# # cp_publisher_one_hot = feature_column.indicator_column(cp_publisher)\n",
    "# # feature_columns.append(cp_publisher_one_hot)\n",
    "\n",
    "# # cp_location = feature_column.categorical_column_with_vocabulary_list('cp_location', vocabulary_list=locs, num_oov_buckets=1)\n",
    "# # cp_location_embedding = feature_column.embedding_column(cp_location, dimension=8)\n",
    "# # feature_columns.append(cp_location_embedding)\n",
    "\n",
    "# # cp_life_hour = feature_column.numeric_column(\"cp_life_hour\")\n",
    "# # cp_life_hour = feature_column.bucketized_column(cp_life_hour, boundaries=[72, 24 * 7, 24 * 14, 24 * 30])\n",
    "# # feature_columns.append(cp_life_hour)\n",
    "\n",
    "# # rs_channel = feature_column.categorical_column_with_vocabulary_list('rs_channel', vocabulary_list=rschannles, num_oov_buckets=1)\n",
    "# # rs_channel_embedding = feature_column.embedding_column(rs_channel, dimension=6)\n",
    "# # feature_columns.append(rs_channel_embedding)\n",
    "\n",
    "# # rs_taginfo = feature_column.categorical_column_with_vocabulary_list('rs_taginfo', vocabulary_list=its, num_oov_buckets=1)\n",
    "# # rs_taginfo_embedding = feature_column.embedding_column(rs_taginfo, dimension=8)\n",
    "# # feature_columns.append(rs_taginfo_embedding)\n",
    "\n",
    "# # rs_taginfo = feature_column.categorical_column_with_hash_bucket('rs_taginfo', hash_bucket_size=30000, dtype=tf.string)\n",
    "# # rs_taginfo_one_hot = feature_column.indicator_column(rs_taginfo)\n",
    "# # feature_columns.append(rs_taginfo_one_hot)\n",
    "# # feature_layer_inputs[\"rs_taginfo\"] = tf.keras.Input(shape=(10,), name=\"rs_taginfo\", dtype=tf.string)\n",
    "# # rs_taginfo_embedding = feature_column.embedding_column(rs_taginfo, dimension=8)\n",
    "# # feature_columns.append(rs_taginfo_embedding)\n",
    "\n",
    "# #构造输入特征\n",
    "# feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\n",
    "# feature_layer_outputs = feature_layer(feature_layer_inputs)\n",
    "# batch_size = 128\n",
    "# buffer_size = 5000\n",
    "# train_dataset = train_dataset.shuffle(buffer_size).batch(batch_size).repeat(1)\n",
    "# test_dataset = test_dataset.shuffle(buffer_size).batch(batch_size)\n",
    "\n",
    "\n",
    "# #定义模型\n",
    "# dense1 = tf.keras.layers.Dense(128, activation='relu')(feature_layer_outputs)\n",
    "# dense2 = tf.keras.layers.Dense(64, activation='relu')(dense1)\n",
    "# dense3 = tf.keras.layers.Dense(1, activation='sigmoid')(dense2)\n",
    "# model = tf.keras.Model(inputs=feature_layer_inputs, outputs=dense3)\n",
    "\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss='binary_crossentropy',\n",
    "#               metrics=['binary_crossentropy', 'accuracy', tf.keras.metrics.AUC()],\n",
    "#               run_eagerly=True)\n",
    "\n",
    "# # 模型训练\n",
    "# model.fit(train_dataset,\n",
    "#           validation_data=test_dataset,\n",
    "#           epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
